\begin{figure*}[t]
  \centering
  \begin{tabular}{@{}cccc@{}}
    \includegraphics[width=0.23\textwidth]{../../assets/figures/average-duration-of-actions.png} &
    \includegraphics[width=0.23\textwidth]{../../assets/figures/average-number-of-action-occurrences-per-video.png} &
    \includegraphics[width=0.23\textwidth]{../../assets/figures/distribution-of-actions-in-dataset.png} &
    \includegraphics[width=0.23\textwidth]{../../assets/figures/temporal-distribution-of-actions.png} \\
    \begin{minipage}{0.23\textwidth}\centering\caption{Average duration of actions in the dataset.}\label{fig:average-duration-of-actions}\end{minipage} &
    \begin{minipage}{0.23\textwidth}\centering\caption{Average Action Count per Video.}\label{fig:average-number-of-action-occurences-per-video}\end{minipage} &
    \begin{minipage}{0.23\textwidth}\centering\caption{Distribution of actions in the dataset.}\label{fig:distribution-of-actions-in-dataset}\end{minipage} &
    \begin{minipage}{0.23\textwidth}\centering\caption{Temporal distribution of actions in the dataset}\label{fig:temporal-distribution-of-actions}\end{minipage}
  \end{tabular}
\end{figure*}

\section{Dataset}

In this section, we present the dataset used in this project, detailing its construction, preparation, and some important statistics. We also compare our dataset to other popular datasets used in temporal action segmentation.

\subsection{Raw Format}

Our dataset was collected during the "Manip Chambery" bouldering event. It consists of videos filmed from two different angles of 10 climbers attempting to complete 2 distinct bouldering blocks (events) each. This resulted in a total of 20 unique climbs, or 40 videos in total, with each video having a duration of 4 minutes.

The climbers' activities during the event are diverse. They may engage in actions such as brushing the holds, observing them, and climbing. These actions occur freely within the 4-minute duration of each video, providing a rich set of diverse activities.

As specified in \cite{section:context}, the videos were annotated by the climbers themselves, and the annotations were provided in raw Excel format. The annotations are segment-level, meaning that for each action, we are given the start time and duration of the activity, along with the corresponding label. Out of the 20 climbs, 11 have been annotated with action segments.

The total duration of the dataset is 2 hours and 40 minutes, of which 1 hour and 28 minutes are annotated, forming the basis of our experiments.

\noindent\textbf{Dataset Limitations.}  
While the dataset provides valuable data for action segmentation tasks, it has several limitations: 

\textbf{1.} The dataset size is relatively small, containing only 40 unique videos.
\textbf{2.} The annotations are provided in Excel format, which is not ideal for modern data science workflows.
\textbf{3.} The annotations themselves are not perfect, as they may be shifted in time, and the videos do not always start precisely at the beginning of the event.
\textbf{4.} Climbers occasionally leave the frame during the video, and in some instances, other people may enter the frame, which introduces noise to the data.

Despite these limitations, this dataset provides an essential foundation for the development and evaluation of action segmentation models in the context of bouldering.
\subsection{Chosen Structure}

We decided to structure the dataset in a way that is specifically suited for temporal action segmentation and classification tasks, with a focus on both ease of use and efficiency for training. This structure is designed to allow for easy scalability as new data is added to the project, an important consideration as the dataset will be updated by individuals who may not have a background in data science.

\begin{verbatim}
    dataset
    +-- videos
    |   +-- video-1
    |   |   +-- frame-1.jpg
    |   |   +-- frame-2.jpg
    |   |   +-- ...
    |   +-- ...
    +-- annotations
    |   +-- video-1.csv
    |   +-- video-2.csv
    |   +-- ...
\end{verbatim}

The advantage of this structure is that it does not require additional processing of the videos or annotations provided by the data collection team, ensuring the dataset can be easily expanded with new annotations or videos in the future.

To optimize for video loading, we store each video as a sequence of JPG frames rather than the video file itself. This avoids the need for decoding during runtime, though it comes at the cost of additional storage space. This structure also makes the dataset more versatile and compatible with various libraries and tools.

\begin{AIbox}{Python Package - Video Dataset.}
  We have developed a Python package, \href{https://github.com/raideno/video-dataset}{https://github.com/raideno/video-dataset}, to support this dataset structure. The package provides utilities for easily loading videos, transforming them into this frame-based format, and handling annotations at the frame level. It is highly customizable and can be adapted to work with different types of datasets.
\end{AIbox}

A related tool, \href{https://github.com/raideno/cached-dataset}{https://github.com/raideno/cached-dataset}, serves as a wrapper for an existing dataset, caching the transformed version on disk. This is particularly useful in avoiding the need to repeatedly extract features during model training or evaluation. We will discuss the details of this tool in the training procedure section.

By utilizing this structure and the provided tools, we were able to quickly load the dataset and begin training the model.

\subsection{Data Exploration}

In this section we are going to explore various aspects and statistics about our dataset.

As the plot in Figure \ref{fig:distribution-of-actions-in-dataset} shows, the dataset contains 5 different actions, with each action occurring a different number of times. The distribution of actions is not uniform, with some actions appearing more frequently than others. This non-uniform distribution can be challenging for the model to learn, as it may lead to biases in the predictions. The dataset is unbalanced

\todo[inline]{Explain the plotst.}

We can also see that a significant portion of the dataset (15\%) frames don't contain any person in the frame. We can also observe that 3\% of the time there is more than one person in the frame which can be a source of noise for the model.
\todo[inline]{Add the plot that talks about the percentage of frames with more than 1 peron.}
In addition to that we can see that 13\% of the climbs are not annotated at all and we thus replaced it by a "Nothing" label.

From \ref{fig:average-number-of-action-occurences-per-video} we  can see that the most recurrent actions are "Lecture" and "Nothing".

From the \ref{fig:average-duration-of-actions} we can see that the longest actions on average are the cleaning, climbing and observation. Even tho the nothing class is the most recurrent it is the shortest in duration on average.

\subsection{Popular Video Datasets}

\label{subsection:popular-video-datasets}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{../../assets/figures/popular-datasets.png}
    \caption{A visualization of popular video datasets.}
    \label{fig:your-label}
\end{figure}

While there are a number of video datasets available for action recognition and segmentation tasks, the overall number remains limited compared to image or text datasets. Video datasets are much more complex due to the added temporal dimension, and therefore require a significantly larger number of samples for effective model training. For instance, while image datasets might need millions of samples, video datasets typically require tens of millions of samples due to the increased complexity.

Some of the most notable video datasets designed for temporal action segmentation include:

\noindent\textbf{50 Salads Dataset.} This dataset features videos of people preparing salads, focusing on fine-grained hand interactions with the environment \cite{50salads-dataset}.

\noindent\textbf{GTEA Dataset.} Consists of Point of View (POV) videos of food preparation, capturing various sub-actions \cite{gtea-dataset}.

\noindent\textbf{Breakfasts Dataset.} Videos of breakfast preparation, with a focus on fine-grained action segmentation \cite{breakfast-dataset}.

\noindent\textbf{Kinetics Family.} Large-scale datasets containing YouTube clips of various human actions, ranging from sports to human-object interactions \cite{kinetics-400-dataset}, \cite{kinetics-600-dataset}, \cite{kinetics-700-dataset}.

\noindent\textbf{Assembly101 Dataset.} Videos of people assembling and disassembling objects, captured from multiple angles, including some POV \cite{assembly101-dataset}.

\noindent\textbf{HowTo100M Dataset.} A massive dataset of instructional (tutorial) videos, covering a broad range of activities \cite{howto100m-dataset}.

\noindent\textbf{Something-Something V2.} Videos of basic actions such as picking up a pen, captured from a POV perspective, focusing on fine-grained interactions \cite{something-something-dataset}.

Note that the three first datasets were specifically designed for temporal action segmentation.

\input{../../assets/tables/popular-datasets.tex}