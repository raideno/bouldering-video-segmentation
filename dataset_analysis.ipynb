{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## **Dataset Analysis**\n",
    "\n",
    "This notebook will analyze and summarize all different aspects of the dataset.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadir/Documents/code/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/nadir/Documents/code/.venv/lib/python3.12/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/Users/nadir/Documents/code/.venv/lib/python3.12/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from enum import IntEnum\n",
    "\n",
    "from video_dataset import VideoDataset\n",
    "from video_dataset.padder import LastValuePadder\n",
    "from video_dataset.dataset import VideoShapeComponents\n",
    "from video_dataset.video import VideoFromVideoFramesDirectory\n",
    "from video_dataset.preprocessor import extract_frames_from_videos\n",
    "from video_dataset.annotations import AnnotationsFromSegmentLevelCsvFileAnnotations\n",
    "\n",
    "from tas_helpers.visualization import SegmentationVisualizer\n",
    "from tas_helpers.scores import repetition_score, order_variation_score\n",
    "from tas_helpers.metrics import mean_over_frames, f1_score, edit_distance\n",
    "\n",
    "from cached_dataset.dataset import DiskCachedDataset\n",
    "\n",
    "from utils import LabelEncoderFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/Users/nadir/Documents/research-project-dataset\"\n",
    "\n",
    "VIDEOS_DIRECTORY_NAME = \"videos\"\n",
    "ANNOTATIONS_DIRECTORY_NAME = \"annotations\"\n",
    "VIDEOS_FRAMES_DIRECTORY_NAME = \"videos_frames\"\n",
    "\n",
    "ALL_IDS_FILE_NAME = \"all_ids.txt\"\n",
    "TESTING_IDS_FILE_NAME = \"testing_ids.txt\"\n",
    "TRAINING_IDS_FILE_NAME = \"training_ids.txt\"\n",
    "VALIDATION_IDS_FILE_NAME = \"validation_ids.txt\"\n",
    "\n",
    "ANNOTATED_IDS_FILE_NAME = \"annotated_ids.txt\"\n",
    "UNANNOTATED_IDS_FILE_NAME = \"unannotated_ids.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoderFactory.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __aggregate_labels(str_labels):\n",
    "    labels = label_encoder.transform(str_labels)\n",
    "    \n",
    "    unique_elements, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    max_count_index = np.argmax(counts)\n",
    "\n",
    "    most_frequent_element = unique_elements[max_count_index]\n",
    "    \n",
    "    return most_frequent_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_videos_dataset = VideoDataset(\n",
    "    annotations_dir=os.path.join(DATASET_PATH, ANNOTATIONS_DIRECTORY_NAME),\n",
    "    videos_dir=os.path.join(DATASET_PATH, VIDEOS_FRAMES_DIRECTORY_NAME),\n",
    "    ids_file=os.path.join(DATASET_PATH, ALL_IDS_FILE_NAME),\n",
    "    segment_size=32,\n",
    "    video_processor=VideoFromVideoFramesDirectory,\n",
    "    annotations_processor=AnnotationsFromSegmentLevelCsvFileAnnotations,\n",
    "    annotations_processor_kwargs={\"fps\": 25, \"delimiter\": \",\"},\n",
    "    video_shape=(VideoShapeComponents.CHANNELS, VideoShapeComponents.TIME, VideoShapeComponents.HEIGHT, VideoShapeComponents.WIDTH),\n",
    "    step=1,\n",
    "    # padder=LastValuePadder(),\n",
    "    annotations_transform=__aggregate_labels,\n",
    "    overlap=0,\n",
    "    allow_undefined_annotations=True,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_videos_dataset = VideoDataset(\n",
    "    annotations_dir=os.path.join(DATASET_PATH, ANNOTATIONS_DIRECTORY_NAME),\n",
    "    videos_dir=os.path.join(DATASET_PATH, VIDEOS_FRAMES_DIRECTORY_NAME),\n",
    "    ids_file=os.path.join(DATASET_PATH, ANNOTATED_IDS_FILE_NAME),\n",
    "    segment_size=VideoDataset.FULL_VIDEO_SEGMENT,\n",
    "    video_processor=VideoFromVideoFramesDirectory,\n",
    "    annotations_processor=AnnotationsFromSegmentLevelCsvFileAnnotations,\n",
    "    annotations_processor_kwargs={\"fps\": 25, \"delimiter\": \",\"},\n",
    "    video_shape=(VideoShapeComponents.CHANNELS, VideoShapeComponents.TIME, VideoShapeComponents.HEIGHT, VideoShapeComponents.WIDTH),\n",
    "    step=1,\n",
    "    # padder=LastValuePadder(),\n",
    "    # annotations_transform=__aggregate_labels,\n",
    "    overlap=0,\n",
    "    allow_undefined_annotations=True,\n",
    "    load_videos=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dataset-size]: 22\n"
     ]
    }
   ],
   "source": [
    "print(f\"[dataset-size]: {len(annotated_videos_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **Repetition Score:**\n",
    "\n",
    "The repetition score ranges between 0 and -1. A higher value such as in our case signifies a higher degree of repetition within the sequences. In simple words, actions usually repeat in the sequence / activity.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[repetition-score]: 0.8131712451488049 ± 0.03650003778074824\n"
     ]
    }
   ],
   "source": [
    "repetition_scores = [repetition_score(annotations) for _, annotations in annotated_videos_dataset]\n",
    "\n",
    "average_repetition_score = np.mean(repetition_scores)\n",
    "std_repetition_score = np.std(repetition_scores)\n",
    "    \n",
    "print(f\"[repetition-score]: {average_repetition_score} ± {std_repetition_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **Order Variation Score:**\n",
    "\n",
    "The repetition score ranges between 0 and -1. A higher value such as in our case signifies a higher degree of repetition within the sequences. In simple words, actions usually repeat in the sequence / activity.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m videos_annotations \u001b[38;5;241m=\u001b[39m [annotations \u001b[38;5;28;01mfor\u001b[39;00m _, annotations \u001b[38;5;129;01min\u001b[39;00m annotated_videos_dataset]\n\u001b[0;32m----> 3\u001b[0m order_variation_scores \u001b[38;5;241m=\u001b[39m \u001b[43morder_variation_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideos_annotations\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/.venv/lib/python3.12/site-packages/tas_helpers/scores/order_variation.py:20\u001b[0m, in \u001b[0;36morder_variation_score\u001b[0;34m(frame_level_videos_annotations)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_videos):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, num_videos):\n\u001b[0;32m---> 20\u001b[0m         edit_dist \u001b[38;5;241m=\u001b[39m \u001b[43medit_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_level_videos_annotations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_level_videos_annotations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m         total_edit_distance \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m edit_dist\n\u001b[1;32m     22\u001b[0m         max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(max_length, \u001b[38;5;28mlen\u001b[39m(frame_level_videos_annotations[i]), \u001b[38;5;28mlen\u001b[39m(frame_level_videos_annotations[j]))\n",
      "File \u001b[0;32m~/Documents/code/.venv/lib/python3.12/site-packages/tas_helpers/metrics/edit_distance.py:19\u001b[0m, in \u001b[0;36medit_distance\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m     17\u001b[0m             dp[i][j] \u001b[38;5;241m=\u001b[39m dp[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m][j \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m             dp[i][j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dp[m][n]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "videos_annotations = [annotations for _, annotations in annotated_videos_dataset]\n",
    "\n",
    "order_variation_scores = order_variation_score(videos_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(order_variation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **Actions Durations:**\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **Segmentations Visualizations Examples:**\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
