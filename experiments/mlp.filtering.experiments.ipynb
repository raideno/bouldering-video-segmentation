{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **MLP Filtering Experiment**\n",
    "\n",
    "This notebook is just a duplicate of MLP experiment where we run the MLP experiment multiple times with all possible filtering combinations in order to see which one performs the best.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "def generate_power_set(input_set, with_empty_set: bool = False):\n",
    "    power_set = list(chain.from_iterable(combinations(input_set, r) for r in range(len(input_set) + 1)))\n",
    "    \n",
    "    if not with_empty_set:\n",
    "        power_set = power_set[1:]\n",
    "        \n",
    "    return power_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from experiments.helpers.preliminary import preliminary, FilteringMode, FilteringOperator\n",
    "\n",
    "filtering_modes = [\n",
    "    [FilteringMode(0)],\n",
    "    *generate_power_set(\n",
    "        [\n",
    "            FilteringMode.NO_PERSONLESS,\n",
    "            FilteringMode.NO_STOPWATCH_CLASS,\n",
    "            FilteringMode.NO_NOTHING_CLASS,\n",
    "            FilteringMode.NO_MULTI_CLASS\n",
    "        ]\n",
    "    ),\n",
    "]\n",
    "\n",
    "filtering_modes = [reduce(lambda x, y: x | y, filtering_mode) for filtering_mode in filtering_modes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[filtering-modes]:   0%|          | 0/16 [00:00<?, ?it/s]Using cache found in /Users/nadir/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[missing-keys]: <All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/nadir/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "Using cache found in /Users/nadir/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "Using cache found in /Users/nadir/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "Using cache found in /Users/nadir/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "Using cache found in /Users/nadir/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: frames for \"climb_1-climber_MoubeAdrian-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_1-climber_MoubeAdrian-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_10-climber_DouglasSophia-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_10-climber_DouglasSophia-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_11-climber_MoubeAdrian-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_11-climber_MoubeAdrian-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_12-climber_MrideEsteban-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_12-climber_MrideEsteban-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_13-climber_FonneLana-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_13-climber_FonneLana-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_14-climber_PlancheLeo-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_14-climber_PlancheLeo-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_15-climber_ChatagonMael-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_15-climber_ChatagonMael-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_16-climber_LyantMargaux-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_16-climber_LyantMargaux-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_17-climber_MuteeMathis-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_17-climber_MuteeMathis-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_18-climber_LegrosNinon-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_19-climber_DouglasSophia-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_19-climber_DouglasSophia-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_2-climber_MrideEsteban-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_2-climber_MrideEsteban-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_20-climber_MasseQuentin-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_3-climber_FonneLana-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_3-climber_FonneLana-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_4-climber_PlancheLeo-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_4-climber_PlancheLeo-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_5-climber_ChatagonMael-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_5-climber_ChatagonMael-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_6-climber_LyantMargaux-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_6-climber_LyantMargaux-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_7-climber_MuteeMathis-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_7-climber_MuteeMathis-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_8-climber_LegrosNinon-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_8-climber_LegrosNinon-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_9-climber_MasseQuentin-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_9-climber_MasseQuentin-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[extractor-yolo]:\n",
      "[extractor-resnet-3d]:\n",
      "[extractor-i3d]:\n",
      "[extractor-clip]:\n",
      "[extractor-x3d_xs]:\n",
      "[extractor-x3d_s]:\n",
      "[extractor-x3d_m]:\n",
      "[extractor-x3d_l]:\n",
      "[extractor-s3d-kinetics]:\n",
      "[extractor-s3d-howto100m]:\n",
      "[extractor-slowfast]:\n",
      "[filtering]: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dataset-mappings-construction]:: 100%|██████████| 4098/4098 [00:00<00:00, 5822.90it/s]\n",
      "[dataset-mappings-construction]:: 100%|██████████| 4098/4098 [00:00<00:00, 15392.30it/s]\n",
      "[training-yolo-1/5]: 100%|██████████| 32/32 [00:10<00:00,  3.13epoch/s, training-loss=1.06, training-accuracy=0.602, validation-loss=1.06, validation-accuracy=0.616, best-validation-accuracy=0.616, best-training-accuracy=0.602]\n",
      "[dataset-mappings-construction]:: 100%|██████████| 4098/4098 [00:00<00:00, 4609.32it/s]\n",
      "[dataset-mappings-construction]:: 100%|██████████| 4098/4098 [00:00<00:00, 13487.48it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "for FILTERING_MODE in tqdm.tqdm(iterable=filtering_modes, desc=\"[filtering-modes]\"):\n",
    "    datasets, filtered_datasets, extractors = preliminary(\n",
    "        filtering_mode=FILTERING_MODE,\n",
    "        filtering_operator=FilteringOperator.OR\n",
    "    )\n",
    "    \n",
    "    initial_size = len(datasets[0])\n",
    "    filtered_size = len(filtered_datasets[0])\n",
    "\n",
    "    reduction_percentage = 100 * (initial_size - filtered_size) / initial_size\n",
    "\n",
    "    print(f\"[filtering]: {reduction_percentage:.2f}%\")\n",
    "    \n",
    "    import torch\n",
    "\n",
    "    class SimpleLinearClassifier(torch.nn.Module):\n",
    "        def __init__(self, input_size, output_size):\n",
    "            super(SimpleLinearClassifier, self).__init__()\n",
    "            \n",
    "            self.network = torch.nn.Sequential(\n",
    "                torch.nn.Flatten(),\n",
    "                torch.nn.LazyLinear(out_features=output_size)\n",
    "            )\n",
    "            \n",
    "        def forward(self, x):\n",
    "            return self.network(x)\n",
    "        \n",
    "    class WrapperDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, dataset, transform=None):\n",
    "            self.dataset = dataset\n",
    "            self.transform = transform\n",
    "            \n",
    "        def __getitem__(self, index):\n",
    "            if self.transform:\n",
    "                return self.transform(self.dataset[index])\n",
    "            else:\n",
    "                return self.dataset[index]\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "        \n",
    "    def transform(sample):\n",
    "        features, annotations, video_id, segment_index = sample\n",
    "        \n",
    "        return features, annotations[0]\n",
    "    \n",
    "    NUMBER_OF_FOLDS = 5\n",
    "    NUMBER_ANNOTATED_VIDEOS = 22\n",
    "\n",
    "    from utils import LabelEncoderFactory\n",
    "\n",
    "    from experiments.helpers.trainer import Trainer\n",
    "    from experiments.helpers.splits_generator import splits_generator\n",
    "    from experiments.helpers.videos_to_indices import videos_to_indices\n",
    "\n",
    "    label_encoder = LabelEncoderFactory.get()\n",
    "\n",
    "    folds_histories: list[dict] = []\n",
    "\n",
    "    for fold_index, folds in enumerate(splits_generator(dataset_length=NUMBER_ANNOTATED_VIDEOS, k=NUMBER_OF_FOLDS)):\n",
    "        histories = {}\n",
    "        \n",
    "        for dataset, extractor in zip(filtered_datasets, extractors):\n",
    "            training_videos_ids, validation_videos_ids = folds\n",
    "        \n",
    "            # TODO: the issue is here, as the cached version is being used, some indices that have been filtered are trying to be reused\n",
    "            training_samples_ids = videos_to_indices(dataset, training_videos_ids)\n",
    "            testing_samples_ids = videos_to_indices(dataset, validation_videos_ids)\n",
    "            \n",
    "            training_dataset = WrapperDataset(torch.utils.data.Subset(dataset, training_samples_ids), transform)\n",
    "            validation_dataset = WrapperDataset(torch.utils.data.Subset(dataset, testing_samples_ids), transform)\n",
    "            \n",
    "            training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=32, shuffle=True)\n",
    "            validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "            linear_classifier = SimpleLinearClassifier(input_size=training_dataset[0][0].shape[0], output_size=len(label_encoder.classes_))\n",
    "            \n",
    "            trainer = Trainer(linear_classifier)\n",
    "            \n",
    "            statistics = trainer.train(training_dataloader, validation_dataloader, title=f\"[training-{extractor.get_name()}-{fold_index + 1}/{NUMBER_OF_FOLDS}]\")\n",
    "            \n",
    "            histories[extractor.get_name()] = statistics\n",
    "            \n",
    "        folds_histories.append(histories)\n",
    "        \n",
    "    # NOTE: For each model we are going to put a plot, a box plot or something to display the variance of the validation accuracy between each fold\n",
    "    models_training_accuracies = {}\n",
    "    models_validation_accuracies = {}\n",
    "\n",
    "    for fold_history in folds_histories:\n",
    "        for extractor_name, details in fold_history.items():\n",
    "            if models_validation_accuracies.get(extractor_name) is None:\n",
    "                models_validation_accuracies[extractor_name] = [details[\"best_validation_accuracy\"]] \n",
    "            else:\n",
    "                models_validation_accuracies[extractor_name].append(details[\"best_validation_accuracy\"])\n",
    "                \n",
    "            if models_training_accuracies.get(extractor_name) is None:\n",
    "                models_training_accuracies[extractor_name] = [details[\"best_training_accuracy\"]] \n",
    "            else:\n",
    "                models_training_accuracies[extractor_name].append(details[\"best_training_accuracy\"])\n",
    "                \n",
    "    models_training_losses = {}\n",
    "    models_validation_losses = {}\n",
    "\n",
    "    for fold_history in folds_histories:\n",
    "        for extractor_name, details in fold_history.items():\n",
    "            if models_validation_losses.get(extractor_name) is None:\n",
    "                models_validation_losses[extractor_name] = [details[\"best_validation_loss\"]] \n",
    "            else:\n",
    "                models_validation_losses[extractor_name].append(details[\"best_validation_loss\"])\n",
    "                \n",
    "            if models_training_losses.get(extractor_name) is None:\n",
    "                models_training_losses[extractor_name] = [details[\"best_training_loss\"]] \n",
    "            else:\n",
    "                models_training_losses[extractor_name].append(details[\"best_training_loss\"])\n",
    "                \n",
    "                \n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # --- --- ---\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.suptitle(f\"Model Performance Across 5 Folds - {\", \".join(FilteringMode.get_str_components(FILTERING_MODE))}\")\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.boxplot(models_validation_accuracies.values(), tick_labels=models_validation_accuracies.keys())\n",
    "    # plt.title(\"Model Performance Across 5 Folds (Accuracy)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.axhline(y=0.9, color='r', linestyle='--', label='90% Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(models_validation_losses.values(), tick_labels=models_validation_losses.keys())\n",
    "    # plt.title(\"Model Performance Across 5 Folds (Loss)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.ylim(0, 1.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    name = \",\".join(FilteringMode.get_str_components(FILTERING_MODE)).replace(\"_\", \"-\").lower() + \".boxplot.png\"\n",
    "\n",
    "    plt.savefig(name)\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "    # --- --- ---\n",
    "\n",
    "    for model_name, accuracies in models_validation_accuracies.items():\n",
    "        average_accuracy = np.mean(accuracies)\n",
    "        min_accuracy, max_accuracy = np.min(accuracies), np.max(accuracies)\n",
    "        std = np.std(accuracies)\n",
    "        print(f\"[{model_name}]: {average_accuracy:.2f} ± {std:.2f}; ({min_accuracy:.2f}, {max_accuracy:.2f})\")\n",
    "\n",
    "    for model_name, losses in models_validation_losses.items():\n",
    "        average_loss = np.mean(losses)\n",
    "        min_loss, max_loss = np.min(losses), np.max(losses)\n",
    "        std = np.std(losses)\n",
    "        print(f\"[{model_name} Loss]: {average_loss:.2f} ± {std:.2f}; ({min_loss:.2f}, {max_loss:.2f})\")\n",
    "        \n",
    "    from matplotlib.lines import Line2D\n",
    "\n",
    "    models_names = models_validation_accuracies.keys()\n",
    "\n",
    "    markers = list(Line2D.markers.keys())\n",
    "    markers = markers[:len(models_names)]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.suptitle(f\"Model Performance Across 5 Folds - {\", \".join(FilteringMode.get_str_components(FILTERING_MODE))}\")\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i, model_name, marker in zip(range(len(models_names)), models_names, markers):\n",
    "        model_average_validation_accuracy = np.mean(models_validation_accuracies[model_name])\n",
    "        model_average_training_accuracy = np.mean(models_training_accuracies[model_name])\n",
    "        \n",
    "        plt.scatter(model_average_validation_accuracy, model_average_training_accuracy, label=model_name, marker=marker, s=100)\n",
    "        # plt.text(model_average_validation_accuracy, model_average_training_accuracy - 0.02, model_name, ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "    plt.xlabel(\"Validation Accuracy\")\n",
    "    plt.ylabel(\"Training Accuracy\")\n",
    "    # plt.title(\"Comparison of Model Accuracies\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i, model_name, marker in zip(range(len(models_names)), models_names, markers):\n",
    "        model_average_validation_loss = np.mean(models_validation_losses[model_name])\n",
    "        model_average_training_loss = np.mean(models_training_losses[model_name])\n",
    "        \n",
    "        plt.scatter(model_average_validation_loss, model_average_training_loss, label=model_name, marker=marker, s=100)\n",
    "        # plt.text(model_average_validation_accuracy, model_average_training_accuracy - 0.02, model_name, ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "    plt.xlabel(\"Validation Loss\")\n",
    "    plt.ylabel(\"Training Loss\")\n",
    "    # plt.title(\"Comparison of Model Losses\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    name = \",\".join(FilteringMode.get_str_components(FILTERING_MODE)).replace(\"_\", \"-\").lower() + \".scatterplot.png\"\n",
    "\n",
    "    plt.savefig(name)\n",
    "    \n",
    "    # plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
