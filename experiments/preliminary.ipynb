{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **0 - Requirements**\n",
    "\n",
    "**NOTE:** If you downloaded the dataset from the github repository, this part can be skipped.\n",
    "\n",
    "In order to run this notebook a dataset is required. To make it simple you'll need to define the following variables in the `helpers.constants` file:\n",
    "- `DATASET_PATH`: This is the base path were all the transformations and other thing happening on the dataset will be done.\n",
    "- `VIDEOS_DIRECTORY_NAME`: This is the directory name inside the `DATASET_PATH` were you put your videos file (.mp4, or .mov, etc).\n",
    "- `ANNOTATIONS_DIRECTORY_NAME`: This is the directory were you put the annotations file, must be csv files named the same way as the corresponding video file (except the extension).\n",
    "- `ANNOTATED_IDS_FILE_NAME`: This is a text file containing the names of the annotated videos.\n",
    "- `UNANNOTATED_IDS_FILE_NAME`: This is a text file containing the names of the unannotated videos.\n",
    "\n",
    "And:\n",
    "- `VIDEOS_FRAMES_DIRECTORY_NAME`: This is a name you need to specify and on which the extracted features of the videos will be stored.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **1- Data Preparation**\n",
    "\n",
    "During this step we'll transform the videos from a video format into a frame by frame format, and thus we'll store each frame of each video in a .pnj file separately.\n",
    "\n",
    "We do this for faster training as loading images is faster than videos. This step can be skipped (a small modification will be required if so).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: frames for \"climb_1-climber_MoubeAdrian-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_1-climber_MoubeAdrian-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_10-climber_DouglasSophia-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_10-climber_DouglasSophia-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_11-climber_MoubeAdrian-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_11-climber_MoubeAdrian-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_12-climber_MrideEsteban-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_12-climber_MrideEsteban-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_13-climber_FonneLana-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_13-climber_FonneLana-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_14-climber_PlancheLeo-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_14-climber_PlancheLeo-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_15-climber_ChatagonMael-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_15-climber_ChatagonMael-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_16-climber_LyantMargaux-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_16-climber_LyantMargaux-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_17-climber_MuteeMathis-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_17-climber_MuteeMathis-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_18-climber_LegrosNinon-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_19-climber_DouglasSophia-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_19-climber_DouglasSophia-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_2-climber_MrideEsteban-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_2-climber_MrideEsteban-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_20-climber_MasseQuentin-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_3-climber_FonneLana-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_3-climber_FonneLana-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_4-climber_PlancheLeo-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_4-climber_PlancheLeo-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_5-climber_ChatagonMael-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_5-climber_ChatagonMael-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_6-climber_LyantMargaux-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_6-climber_LyantMargaux-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_7-climber_MuteeMathis-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_7-climber_MuteeMathis-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_8-climber_LegrosNinon-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_8-climber_LegrosNinon-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_9-climber_MasseQuentin-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_9-climber_MasseQuentin-bloc_1-angle_profile\" already exist. skipping extraction.\n"
     ]
    }
   ],
   "source": [
    "from video_dataset.preprocessor import extract_frames_from_videos\n",
    "\n",
    "from helpers.constants import \\\n",
    "    DATASET_PATH, \\\n",
    "    VIDEOS_DIRECTORY_NAME, \\\n",
    "    VIDEOS_FRAMES_DIRECTORY_NAME \\\n",
    "    \n",
    "extract_frames_from_videos(\n",
    "    videos_dir=os.path.join(DATASET_PATH, VIDEOS_DIRECTORY_NAME),\n",
    "    output_dir=os.path.join(DATASET_PATH, VIDEOS_FRAMES_DIRECTORY_NAME),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **2- Feature Extraction**\n",
    "\n",
    "During this step we are going to import a set of predefined feature extractors which is located in the `helpers.constants` module. We'll then extract features from the annotated videos using this feature extractors.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.constants import \\\n",
    "    FEATURES_EXTRACTORS, \\\n",
    "    DATASET_PATH, \\\n",
    "    VIDEOS_FRAMES_DIRECTORY_NAME, \\\n",
    "    ANNOTATIONS_DIRECTORY_NAME, \\\n",
    "    ANNOTATED_IDS_FILE_NAME\n",
    "from utils import LabelEncoderFactory\n",
    "from cached_dataset import DiskCachedDataset\n",
    "\n",
    "from video_dataset import VideoDataset, VideoShapeComponents\n",
    "from video_dataset.video import VideoFromVideoFramesDirectory\n",
    "from video_dataset.annotations import AnnotationsFromSegmentLevelCsvFileAnnotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoderFactory.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returns_transform(sample):\n",
    "    # sample keys: 'frames', 'annotations', 'video_index', 'video_id', 'starting_frame_number_in_video', 'segment_index'\n",
    "    \n",
    "    return sample[\"frames\"], sample[\"annotations\"], sample[\"video_id\"], sample[\"segment_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __aggregate_labels(string_labels):\n",
    "    \"\"\"\n",
    "    Given a list of string labels, returns the most frequent label and the number of unique labels.\n",
    "    \n",
    "    NOTE: The number of unique labels might be used later to determine if a video segment contain a transition in action or not.\n",
    "    \"\"\"\n",
    "    labels = label_encoder.transform(string_labels)\n",
    "    \n",
    "    unique_elements, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    max_count_index = np.argmax(counts)\n",
    "\n",
    "    most_frequent_element = unique_elements[max_count_index]\n",
    "    \n",
    "    return most_frequent_element, len(unique_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[extractor-yolo]:\n",
      "[extractor-dino]:\n",
      "[extractor-resnet-3d]:\n",
      "[extractor-i3d]:\n",
      "[extractor-clip]:\n",
      "[extractor-x3d_xs]:\n",
      "[extractor-x3d_s]:\n",
      "[extractor-x3d_m]:\n",
      "[extractor-x3d_l]:\n",
      "[extractor-s3d-kinetics]:\n",
      "[extractor-s3d-howto100m]:\n",
      "[extractor-slowfast]:\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "\n",
    "for extractor in FEATURES_EXTRACTORS:\n",
    "    segment_size = 32\n",
    "    \n",
    "    step = VideoDataset.compute_step(segment_size, extractor.get_required_number_of_frames())\n",
    "\n",
    "    dataset = VideoDataset(\n",
    "        annotations_dir=os.path.join(DATASET_PATH, ANNOTATIONS_DIRECTORY_NAME),\n",
    "        videos_dir=os.path.join(DATASET_PATH, VIDEOS_FRAMES_DIRECTORY_NAME),\n",
    "        ids_file=os.path.join(DATASET_PATH, ANNOTATED_IDS_FILE_NAME),\n",
    "        segment_size=segment_size,\n",
    "        step=step,\n",
    "        video_processor=VideoFromVideoFramesDirectory,\n",
    "        annotations_processor=AnnotationsFromSegmentLevelCsvFileAnnotations,\n",
    "        annotations_processor_kwargs={\"fps\": 25, \"delimiter\": \",\"},\n",
    "        video_shape=(VideoShapeComponents.CHANNELS, VideoShapeComponents.TIME, VideoShapeComponents.HEIGHT, VideoShapeComponents.WIDTH),\n",
    "        frames_transform=extractor.transform_and_extract,\n",
    "        annotations_transform=__aggregate_labels,\n",
    "        verbose=False,\n",
    "        return_transform=returns_transform\n",
    "    )\n",
    "\n",
    "    print(f\"[extractor-{extractor.get_name()}]:\")\n",
    "\n",
    "    disk_cached_dataset = DiskCachedDataset.load_dataset_or_cache_it(\n",
    "        dataset=dataset, \n",
    "        base_path=os.path.join(DATASET_PATH, \"features\", extractor.get_name()),\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    datasets.append(disk_cached_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **3- Data Filtering**\n",
    "\n",
    "Now we are going to filter the dataset(s) and get rid of the segments were:\n",
    "- The \"nothing\" class is present, meaning they are not annotated.\n",
    "- No person is present in more than half the segment.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_dataset_index = FEATURES_EXTRACTORS.index(next(filter(lambda x: x.get_name() == \"yolo\", FEATURES_EXTRACTORS)))\n",
    "\n",
    "yolo_dataset = datasets[yolo_dataset_index]\n",
    "\n",
    "# --- --- ---\n",
    "\n",
    "def extract_segments_without_classless_indices(yolo_dataset, classes: list[str]):\n",
    "    label_encoder = LabelEncoderFactory.get()\n",
    "    \n",
    "    return [i for i in range(len(yolo_dataset)) if yolo_dataset[i][1] not in label_encoder.transform(classes)]\n",
    "\n",
    "def extract_segments_with_persons_indices(yolo_dataset):\n",
    "    label_encoder = LabelEncoderFactory.get()\n",
    "    \n",
    "    # NOTE: we chose 4 as we take a sample of 32 frames, we then subsample 8 and extract the yolo features from them, thus 4 is the half of 8\n",
    "    return [i for i in range(len(dataset)) if torch.count_nonzero(torch.sum(yolo_dataset[i][0], dim=1)) >= 4]\n",
    "\n",
    "# NOTE: this is the indices to keep\n",
    "filtered_indices = list(set(extract_segments_without_classless_indices(yolo_dataset, [\"nothing\"]) + extract_segments_with_persons_indices(yolo_dataset)))\n",
    "\n",
    "# --- --- ---\n",
    "\n",
    "filtered_datasets = [torch.utils.data.Subset(dataset, filtered_indices) for dataset in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[len(datasets[0])]: 4098\n",
      "[len(filtered_datasets[0])]: 3995\n",
      "[difference]: 103\n",
      "--- --- ---\n",
      "[percentage]: 2.51%\n"
     ]
    }
   ],
   "source": [
    "print(f\"[len(datasets[0])]: {len(datasets[0])}\")\n",
    "print(f\"[len(filtered_datasets[0])]: {len(filtered_datasets[0])}\")\n",
    "print(f\"[difference]: {len(datasets[0]) - len(filtered_datasets[0])}\")\n",
    "print(f\"--- --- ---\")\n",
    "print(f\"[percentage]: {(len(datasets[0]) - len(filtered_datasets[0])) / len(datasets[0]) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
