{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **0 - Requirements**\n",
    "\n",
    "**NOTE:** If you downloaded the dataset from the github repository, this part can be skipped.\n",
    "\n",
    "In order to run this notebook a dataset is required. To make it simple you'll need to define the following variables in the `helpers.constants` file:\n",
    "- `DATASET_PATH`: This is the base path were all the transformations and other thing happening on the dataset will be done.\n",
    "- `VIDEOS_DIRECTORY_NAME`: This is the directory name inside the `DATASET_PATH` were you put your videos file (.mp4, or .mov, etc).\n",
    "- `ANNOTATIONS_DIRECTORY_NAME`: This is the directory were you put the annotations file, must be csv files named the same way as the corresponding video file (except the extension).\n",
    "- `ANNOTATED_IDS_FILE_NAME`: This is a text file containing the names of the annotated videos.\n",
    "- `UNANNOTATED_IDS_FILE_NAME`: This is a text file containing the names of the unannotated videos.\n",
    "\n",
    "And:\n",
    "- `VIDEOS_FRAMES_DIRECTORY_NAME`: This is a name you need to specify and on which the extracted features of the videos will be stored.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **1- Data Preparation**\n",
    "\n",
    "During this step we'll transform the videos from a video format into a frame by frame format, and thus we'll store each frame of each video in a .pnj file separately.\n",
    "\n",
    "We do this for faster training as loading images is faster than videos. This step can be skipped (a small modification will be required if so).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from video_dataset.preprocessor import extract_frames_from_videos\n",
    "\n",
    "from experiments.helpers.constants import \\\n",
    "    DATASET_PATH, \\\n",
    "    VIDEOS_DIRECTORY_NAME, \\\n",
    "    VIDEOS_FRAMES_DIRECTORY_NAME \\\n",
    "    \n",
    "extract_frames_from_videos(\n",
    "    videos_dir=os.path.join(DATASET_PATH, VIDEOS_DIRECTORY_NAME),\n",
    "    output_dir=os.path.join(DATASET_PATH, VIDEOS_FRAMES_DIRECTORY_NAME),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **2- Feature Extraction**\n",
    "\n",
    "During this step we are going to import a set of predefined feature extractors which is located in the `helpers.constants` module. We'll then extract features from the annotated videos using this feature extractors.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadir/Documents/code/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/nadir/Documents/code/.venv/lib/python3.12/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/Users/nadir/Documents/code/.venv/lib/python3.12/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "Using cache found in /Users/nadir/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/Users/nadir/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/Users/nadir/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/Users/nadir/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "Using cache found in /Users/nadir/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[missing-keys]: <All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/nadir/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "Using cache found in /Users/nadir/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "Using cache found in /Users/nadir/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "Using cache found in /Users/nadir/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "Using cache found in /Users/nadir/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "from cached_dataset import DiskCachedDataset\n",
    "\n",
    "from video_dataset import VideoDataset, VideoShapeComponents\n",
    "from video_dataset.video import VideoFromVideoFramesDirectory\n",
    "from video_dataset.annotations import AnnotationsFromSegmentLevelCsvFileAnnotations\n",
    "\n",
    "from experiments.helpers.constants import \\\n",
    "    FEATURES_EXTRACTORS, \\\n",
    "    DATASET_PATH, \\\n",
    "    VIDEOS_FRAMES_DIRECTORY_NAME, \\\n",
    "    ANNOTATIONS_DIRECTORY_NAME, \\\n",
    "    ANNOTATED_IDS_FILE_NAME\n",
    "from bouldering_video_segmentation.utils import LabelEncoderFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoderFactory.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returns_transform(sample):\n",
    "    # sample keys: 'frames', 'annotations', 'video_index', 'video_id', 'starting_frame_number_in_video', 'segment_index'\n",
    "    \n",
    "    return sample[\"frames\"], sample[\"annotations\"], sample[\"video_id\"], sample[\"segment_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def __aggregate_labels(string_labels):\n",
    "    \"\"\"\n",
    "    Given a list of string labels, returns the most frequent label and the number of unique labels.\n",
    "    \n",
    "    NOTE: The number of unique labels might be used later to determine if a video segment contain a transition in action or not.\n",
    "    \"\"\"\n",
    "    labels = label_encoder.transform(string_labels)\n",
    "    \n",
    "    unique_elements, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    max_count_index = np.argmax(counts)\n",
    "\n",
    "    most_frequent_element = unique_elements[max_count_index]\n",
    "    \n",
    "    return most_frequent_element, len(unique_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[extractor-yolo]:\n",
      "[extractor-dino]:\n",
      "[extractor-r3d]:\n",
      "[extractor-i3d]:\n",
      "[extractor-clip]:\n",
      "[extractor-x3d-xs]:\n",
      "[extractor-x3d-s]:\n",
      "[extractor-x3d-m]:\n",
      "[extractor-x3d-l]:\n",
      "[extractor-s3d-k]:\n",
      "[extractor-s3d-h]:\n",
      "[extractor-slowfast]:\n",
      "[extractor-vivit]:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "datasets = []\n",
    "\n",
    "for extractor in FEATURES_EXTRACTORS:\n",
    "    segment_size = 32\n",
    "    \n",
    "    step = VideoDataset.compute_step(segment_size, extractor.get_required_number_of_frames())\n",
    "\n",
    "    dataset = VideoDataset(\n",
    "        annotations_dir=os.path.join(DATASET_PATH, ANNOTATIONS_DIRECTORY_NAME),\n",
    "        videos_dir=os.path.join(DATASET_PATH, VIDEOS_FRAMES_DIRECTORY_NAME),\n",
    "        ids_file=os.path.join(DATASET_PATH, ANNOTATED_IDS_FILE_NAME),\n",
    "        segment_size=segment_size,\n",
    "        step=step,\n",
    "        video_processor=VideoFromVideoFramesDirectory,\n",
    "        annotations_processor=AnnotationsFromSegmentLevelCsvFileAnnotations,\n",
    "        annotations_processor_kwargs={\"fps\": 25, \"delimiter\": \",\"},\n",
    "        video_shape=(VideoShapeComponents.CHANNELS, VideoShapeComponents.TIME, VideoShapeComponents.HEIGHT, VideoShapeComponents.WIDTH),\n",
    "        frames_transform=extractor.transform_and_extract,\n",
    "        annotations_transform=__aggregate_labels,\n",
    "        verbose=False,\n",
    "        return_transform=returns_transform\n",
    "    )\n",
    "\n",
    "    print(f\"[extractor-{extractor.get_name()}]:\")\n",
    "\n",
    "    disk_cached_dataset = DiskCachedDataset.load_dataset_or_cache_it(\n",
    "        dataset=dataset, \n",
    "        base_path=os.path.join(DATASET_PATH, \"features\", extractor.get_name()),\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    datasets.append(disk_cached_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **3- Data Filtering**\n",
    "\n",
    "Now we are going to filter the dataset(s) and get rid of the segments were:\n",
    "- The \"nothing\" class is present, meaning they are not annotated.\n",
    "- No person is present in more than half the segment.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "#### **Notice:**\n",
    "\n",
    "If you take a look at the `experiments/helpers/preliminary.py` file, you'll notice that the filtering logic is different and more customizable than the one below.\n",
    "\n",
    "The `experiments/helpers/preliminary.py` is basically a python file that includes a function which does all what is done in this notebook, it is used later in the experiments to not load up the notebooks with code.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "yolo_dataset_index = FEATURES_EXTRACTORS.index(next(filter(lambda x: x.get_name() == \"yolo\", FEATURES_EXTRACTORS)))\n",
    "\n",
    "yolo_dataset = datasets[yolo_dataset_index]\n",
    "\n",
    "# --- --- ---\n",
    "\n",
    "def extract_segments_without_classless_indices(yolo_dataset, classes: list[str]):\n",
    "    label_encoder = LabelEncoderFactory.get()\n",
    "    \n",
    "    return [i for i in range(len(yolo_dataset)) if yolo_dataset[i][1] not in label_encoder.transform(classes)]\n",
    "\n",
    "def extract_segments_with_persons_indices(yolo_dataset):\n",
    "    label_encoder = LabelEncoderFactory.get()\n",
    "    \n",
    "    # NOTE: we chose 4 as we take a sample of 32 frames, we then subsample 8 and extract the yolo features from them, thus 4 is the half of 8\n",
    "    return [i for i in range(len(dataset)) if torch.count_nonzero(torch.sum(yolo_dataset[i][0], dim=1)) >= 4]\n",
    "\n",
    "# NOTE: this is the indices to keep\n",
    "filtered_indices = list(set(extract_segments_without_classless_indices(yolo_dataset, [\"nothing\"]) + extract_segments_with_persons_indices(yolo_dataset)))\n",
    "\n",
    "# --- --- ---\n",
    "\n",
    "filtered_datasets = [torch.utils.data.Subset(dataset, filtered_indices) for dataset in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[len(datasets[0])]: 4098\n",
      "[len(filtered_datasets[0])]: 3995\n",
      "[difference]: 103\n",
      "--- --- ---\n",
      "[percentage]: 2.51%\n"
     ]
    }
   ],
   "source": [
    "print(f\"[len(datasets[0])]: {len(datasets[0])}\")\n",
    "print(f\"[len(filtered_datasets[0])]: {len(filtered_datasets[0])}\")\n",
    "print(f\"[difference]: {len(datasets[0]) - len(filtered_datasets[0])}\")\n",
    "print(f\"--- --- ---\")\n",
    "print(f\"[percentage]: {(len(datasets[0]) - len(filtered_datasets[0])) / len(datasets[0]) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **4- Bonus**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[extractor-yolo]: torch.Size([8, 34])\n",
      "[extractor-dino]: torch.Size([8, 384])\n",
      "[extractor-resnet-3d]: torch.Size([2048])\n",
      "[extractor-i3d]: torch.Size([1024])\n",
      "[extractor-clip]: torch.Size([8, 512])\n",
      "[extractor-x3d_xs]: torch.Size([2048])\n",
      "[extractor-x3d_s]: torch.Size([2048])\n",
      "[extractor-x3d_m]: torch.Size([2048])\n",
      "[extractor-x3d_l]: torch.Size([2048])\n",
      "[extractor-s3d-kinetics]: torch.Size([1024])\n",
      "[extractor-s3d-howto100m]: torch.Size([1024])\n",
      "[extractor-slowfast]: torch.Size([2304])\n",
      "[extractor-vivit]: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "\n",
    "for extractor in FEATURES_EXTRACTORS:\n",
    "    segment_size = 32\n",
    "    \n",
    "    step = VideoDataset.compute_step(segment_size, extractor.get_required_number_of_frames())\n",
    "\n",
    "    dataset = VideoDataset(\n",
    "        annotations_dir=os.path.join(DATASET_PATH, ANNOTATIONS_DIRECTORY_NAME),\n",
    "        videos_dir=os.path.join(DATASET_PATH, VIDEOS_FRAMES_DIRECTORY_NAME),\n",
    "        ids_file=os.path.join(DATASET_PATH, ANNOTATED_IDS_FILE_NAME),\n",
    "        segment_size=segment_size,\n",
    "        step=step,\n",
    "        video_processor=VideoFromVideoFramesDirectory,\n",
    "        annotations_processor=AnnotationsFromSegmentLevelCsvFileAnnotations,\n",
    "        annotations_processor_kwargs={\"fps\": 25, \"delimiter\": \",\"},\n",
    "        video_shape=(VideoShapeComponents.CHANNELS, VideoShapeComponents.TIME, VideoShapeComponents.HEIGHT, VideoShapeComponents.WIDTH),\n",
    "        frames_transform=extractor.transform_and_extract,\n",
    "        annotations_transform=__aggregate_labels,\n",
    "        verbose=False,\n",
    "        return_transform=returns_transform\n",
    "    )\n",
    "\n",
    "    print(f\"[extractor-{extractor.get_name()}]: {dataset[0][0].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
