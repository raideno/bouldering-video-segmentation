{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **LSTM Experiment**\n",
    "\n",
    "In this notebook we are going to train a simple LSTM classifier on the climbing dataset.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **1- Preliminary**\n",
    "\n",
    "We first do a preliminary work to prepare the datasets. In order to know more about this, please read the `experiments/preliminary.ipynb` notebook.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.helpers.preliminary import preliminary, FilteringMode, FilteringOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERING_MODE = FilteringMode(0)\n",
    "# FILTERING_MODE = FilteringMode.NO_PERSONLESS\n",
    "# FILTERING_MODE = FilteringMode.NO_PERSONLESS | FilteringMode.NO_NOTHING_CLASS\n",
    "FILTERING_MODE = FilteringMode.NO_PERSONLESS | FilteringMode.NO_NOTHING_CLASS | FilteringMode.NO_STOPWATCH_CLASS\n",
    "# FILTERING_MODE = FilteringMode.NO_PERSONLESS | FilteringMode.NO_STOPWATCH_CLASS | FilteringMode.NO_NOTHING_CLASS | FilteringMode.NO_MULTI_CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[extractor-yolo]:\n",
      "[extractor-resnet-3d]:\n",
      "[extractor-i3d]:\n",
      "[extractor-clip]:\n",
      "[extractor-x3d_xs]:\n",
      "[extractor-x3d_s]:\n",
      "[extractor-x3d_m]:\n",
      "[extractor-x3d_l]:\n",
      "[extractor-s3d-kinetics]:\n",
      "[extractor-s3d-howto100m]:\n",
      "[extractor-slowfast]:\n"
     ]
    }
   ],
   "source": [
    "datasets, filtered_datasets, extractors = preliminary(\n",
    "    filtering_mode=FILTERING_MODE,\n",
    "    filtering_operator=FilteringOperator.OR,\n",
    "    ignore_frames_extraction=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[filtering]: 21.43%\n"
     ]
    }
   ],
   "source": [
    "initial_size = len(datasets[0])\n",
    "filtered_size = len(filtered_datasets[0])\n",
    "\n",
    "reduction_percentage = 100 * (initial_size - filtered_size) / initial_size\n",
    "\n",
    "print(f\"[filtering]: {reduction_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **2- Data Adaptation**\n",
    "\n",
    "The dataset is structured in term of segments, we need to group and order the segments by videos in order to pass the whole video to an LSTM.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from experiments.helpers.full_videos_features_dataset import FullVideosFeaturesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(sample):\n",
    "    features, annotations, video_id = sample\n",
    "    \n",
    "    return torch.stack(features), torch.tensor(np.array(annotations)[0:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3220/3220 [00:00<00:00, 13522.77it/s]\n",
      "100%|██████████| 3220/3220 [00:00<00:00, 4468.57it/s]\n",
      "100%|██████████| 3220/3220 [00:00<00:00, 4775.49it/s]\n",
      "100%|██████████| 3220/3220 [00:00<00:00, 5816.99it/s]\n",
      "100%|██████████| 3220/3220 [00:00<00:00, 6872.22it/s]\n",
      "100%|██████████| 3220/3220 [00:00<00:00, 8030.21it/s]\n",
      "100%|██████████| 3220/3220 [00:00<00:00, 7852.03it/s]\n",
      "100%|██████████| 3220/3220 [00:00<00:00, 7265.49it/s]\n",
      "100%|██████████| 3220/3220 [00:00<00:00, 6806.81it/s]\n",
      "100%|██████████| 3220/3220 [00:00<00:00, 5013.37it/s]\n",
      "100%|██████████| 3220/3220 [00:00<00:00, 7175.01it/s]\n"
     ]
    }
   ],
   "source": [
    "videos_datasets = [\n",
    "    FullVideosFeaturesDataset(\n",
    "        dataset=dataset,\n",
    "        transform=transform,\n",
    "        verbose=True    \n",
    "    ) for dataset in filtered_datasets\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **3- Model Definition**\n",
    "\n",
    "Now we define the globally temporal aware models and their training functions.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bouldering_video_segmentation.models import FullVideoLstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **4- Training**\n",
    "\n",
    "We'll train the different models.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            return self.transform(self.dataset[index])\n",
    "        else:\n",
    "            return self.dataset[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(sample):\n",
    "    features, annotations = sample\n",
    "    \n",
    "    annotations = torch.nn.functional.one_hot(annotations, num_classes=5).float()\n",
    "    \n",
    "    return features, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    features, labels = zip(*batch)\n",
    "    \n",
    "    features_padded = pad_sequence(features, batch_first=True, padding_value=0)\n",
    "    \n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
    "    \n",
    "    lengths = torch.tensor([f.shape[0] for f in features])\n",
    "    \n",
    "    return features_padded, labels_padded, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[training-yolo-1/5]:   0%|          | 0/32 [00:00<?, ?epoch/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 43\u001b[0m\n\u001b[1;32m     39\u001b[0m     model \u001b[38;5;241m=\u001b[39m GloballyTemporalAwareModel(input_size, hidden_size, output_size, num_layers, dropout)    \n\u001b[1;32m     41\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(model)\n\u001b[0;32m---> 43\u001b[0m     statistics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[training-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfold_index\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mNUMBER_OF_FOLDS\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     histories[extractor\u001b[38;5;241m.\u001b[39mget_name()] \u001b[38;5;241m=\u001b[39m statistics\n\u001b[1;32m     47\u001b[0m folds_histories\u001b[38;5;241m.\u001b[39mappend(histories)\n",
      "File \u001b[0;32m~/Documents/code/experiments/helpers/trainer.py:87\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, training_dataloader, testing_dataloader, title)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(iterable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m32\u001b[39m), desc\u001b[38;5;241m=\u001b[39mtitle \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[training]\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m progress_bar:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[0;32m---> 87\u001b[0m         training_loss, training_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__train_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m         validation_loss, validation_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate(testing_dataloader)\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;66;03m# NOTE: store history\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code/experiments/helpers/trainer.py:21\u001b[0m, in \u001b[0;36mTrainer.__train_one_epoch\u001b[0;34m(self, training_dataloader, learning_rate)\u001b[0m\n\u001b[1;32m     18\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m features, labels \u001b[38;5;129;01min\u001b[39;00m training_dataloader:\n\u001b[1;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     24\u001b[0m     features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "NUMBER_OF_FOLDS = 5\n",
    "NUMBER_ANNOTATED_VIDEOS = 22\n",
    "\n",
    "from bouldering_video_segmentation.utils import LabelEncoderFactory\n",
    "\n",
    "from experiments.helpers.trainer import Trainer\n",
    "from experiments.helpers.splits_generator import splits_generator\n",
    "from experiments.helpers.videos_to_indices import videos_to_indices\n",
    "\n",
    "hidden_size = 128\n",
    "output_size = 5\n",
    "num_layers = 1\n",
    "dropout = 0.0\n",
    "\n",
    "folds_histories: list[dict] = []\n",
    "\n",
    "for fold_index, folds in enumerate(splits_generator(dataset_length=NUMBER_ANNOTATED_VIDEOS, k=NUMBER_OF_FOLDS)):\n",
    "    histories = {}\n",
    "    \n",
    "    for dataset, extractor in zip(videos_datasets, extractors):\n",
    "        training_videos_ids, validation_videos_ids = folds\n",
    "    \n",
    "        training_dataset = WrapperDataset(torch.utils.data.Subset(dataset, training_videos_ids), transform=transform)\n",
    "        validation_dataset = WrapperDataset(torch.utils.data.Subset(dataset, validation_videos_ids), transform=transform)\n",
    "        \n",
    "        training_dataloader = torch.utils.data.DataLoader(\n",
    "            training_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "        validation_dataloader = torch.utils.data.DataLoader(\n",
    "            validation_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        if dataset[0][0].dim() == 3:\n",
    "            input_size = dataset[0][0].shape[1] * dataset[0][0].shape[2]\n",
    "        else:\n",
    "            input_size = dataset[0][0].shape[1]\n",
    "        \n",
    "        model = FullVideoLstm(input_size, hidden_size, output_size, num_layers, dropout)    \n",
    "            \n",
    "        trainer = Trainer(model)\n",
    "        \n",
    "        statistics = trainer.train(training_dataloader, validation_dataloader, title=f\"[training-{extractor.get_name()}-{fold_index + 1}/{NUMBER_OF_FOLDS}]\")\n",
    "        \n",
    "        histories[extractor.get_name()] = statistics\n",
    "        \n",
    "    folds_histories.append(histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **5- Results**\n",
    "\n",
    "Below we are going to display the training results for each model.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
