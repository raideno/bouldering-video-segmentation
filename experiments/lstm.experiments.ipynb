{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **0 - Requirements**\n",
    "\n",
    "**NOTE:** If you downloaded the dataset from the github repository, this part can be skipped.\n",
    "\n",
    "In order to run this notebook a dataset is required. To make it simple you'll need to define the following variables in the `helpers.constants` file:\n",
    "- `DATASET_PATH`: This is the base path were all the transformations and other thing happening on the dataset will be done.\n",
    "- `VIDEOS_DIRECTORY_NAME`: This is the directory name inside the `DATASET_PATH` were you put your videos file (.mp4, or .mov, etc).\n",
    "- `ANNOTATIONS_DIRECTORY_NAME`: This is the directory were you put the annotations file, must be csv files named the same way as the corresponding video file (except the extension).\n",
    "- `ANNOTATED_IDS_FILE_NAME`: This is a text file containing the names of the annotated videos.\n",
    "- `UNANNOTATED_IDS_FILE_NAME`: This is a text file containing the names of the unannotated videos.\n",
    "\n",
    "And:\n",
    "- `VIDEOS_FRAMES_DIRECTORY_NAME`: This is a name you need to specify and on which the extracted features of the videos will be stored.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **1- Data Preparation**\n",
    "\n",
    "During this step we'll transform the videos from a video format into a frame by frame format, and thus we'll store each frame of each video in a .pnj file separately.\n",
    "\n",
    "We do this for faster training as loading images is faster than videos. This step can be skipped (a small modification will be required if so).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: frames for \"climb_1-climber_MoubeAdrian-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_1-climber_MoubeAdrian-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_10-climber_DouglasSophia-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_10-climber_DouglasSophia-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_11-climber_MoubeAdrian-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_11-climber_MoubeAdrian-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_12-climber_MrideEsteban-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_12-climber_MrideEsteban-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_13-climber_FonneLana-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_13-climber_FonneLana-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_14-climber_PlancheLeo-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_14-climber_PlancheLeo-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_15-climber_ChatagonMael-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_15-climber_ChatagonMael-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_16-climber_LyantMargaux-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_16-climber_LyantMargaux-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_17-climber_MuteeMathis-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_17-climber_MuteeMathis-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_18-climber_LegrosNinon-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_19-climber_DouglasSophia-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_19-climber_DouglasSophia-bloc_2-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_2-climber_MrideEsteban-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_2-climber_MrideEsteban-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_20-climber_MasseQuentin-bloc_2-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_3-climber_FonneLana-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_3-climber_FonneLana-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_4-climber_PlancheLeo-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_4-climber_PlancheLeo-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_5-climber_ChatagonMael-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_5-climber_ChatagonMael-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_6-climber_LyantMargaux-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_6-climber_LyantMargaux-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_7-climber_MuteeMathis-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_7-climber_MuteeMathis-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_8-climber_LegrosNinon-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_8-climber_LegrosNinon-bloc_1-angle_profile\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_9-climber_MasseQuentin-bloc_1-angle_face\" already exist. skipping extraction.\n",
      "[INFO]: frames for \"climb_9-climber_MasseQuentin-bloc_1-angle_profile\" already exist. skipping extraction.\n"
     ]
    }
   ],
   "source": [
    "from video_dataset.preprocessor import extract_frames_from_videos\n",
    "\n",
    "from helpers.constants import \\\n",
    "    DATASET_PATH, \\\n",
    "    VIDEOS_DIRECTORY_NAME, \\\n",
    "    VIDEOS_FRAMES_DIRECTORY_NAME \\\n",
    "    \n",
    "extract_frames_from_videos(\n",
    "    videos_dir=os.path.join(DATASET_PATH, VIDEOS_DIRECTORY_NAME),\n",
    "    output_dir=os.path.join(DATASET_PATH, VIDEOS_FRAMES_DIRECTORY_NAME),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **2- Feature Extraction**\n",
    "\n",
    "During this step we are going to import a set of predefined feature extractors which is located in the `helpers.constants` module. We'll then extract features from the annotated videos using this feature extractors.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.constants import \\\n",
    "    FEATURES_EXTRACTORS, \\\n",
    "    DATASET_PATH, \\\n",
    "    VIDEOS_FRAMES_DIRECTORY_NAME, \\\n",
    "    ANNOTATIONS_DIRECTORY_NAME, \\\n",
    "    ANNOTATED_IDS_FILE_NAME\n",
    "from utils import LabelEncoderFactory\n",
    "from cached_dataset import DiskCachedDataset\n",
    "\n",
    "from video_dataset import VideoDataset, VideoShapeComponents\n",
    "from video_dataset.video import VideoFromVideoFramesDirectory\n",
    "from video_dataset.annotations import AnnotationsFromSegmentLevelCsvFileAnnotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoderFactory.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returns_transform(sample):\n",
    "    # sample keys: 'frames', 'annotations', 'video_index', 'video_id', 'starting_frame_number_in_video', 'segment_index'\n",
    "    \n",
    "    return sample[\"frames\"], sample[\"annotations\"], sample[\"video_id\"], sample[\"segment_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __aggregate_labels(string_labels):\n",
    "    \"\"\"\n",
    "    Given a list of string labels, returns the most frequent label and the number of unique labels.\n",
    "    \n",
    "    NOTE: The number of unique labels might be used later to determine if a video segment contain a transition in action or not.\n",
    "    \"\"\"\n",
    "    labels = label_encoder.transform(string_labels)\n",
    "    \n",
    "    unique_elements, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    max_count_index = np.argmax(counts)\n",
    "\n",
    "    most_frequent_element = unique_elements[max_count_index]\n",
    "    \n",
    "    return most_frequent_element, len(unique_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[extractor-yolo]:\n",
      "[extractor-dino]:\n",
      "[extractor-resnet-3d]:\n",
      "[extractor-i3d]:\n",
      "[extractor-clip]:\n",
      "[extractor-x3d_xs]:\n",
      "[extractor-x3d_s]:\n",
      "[extractor-x3d_m]:\n",
      "[extractor-x3d_l]:\n",
      "[extractor-s3d-kinetics]:\n",
      "[extractor-s3d-howto100m]:\n",
      "[extractor-slowfast]:\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "\n",
    "for extractor in FEATURES_EXTRACTORS:\n",
    "    segment_size = 32\n",
    "    \n",
    "    step = VideoDataset.compute_step(segment_size, extractor.get_required_number_of_frames())\n",
    "\n",
    "    dataset = VideoDataset(\n",
    "        annotations_dir=os.path.join(DATASET_PATH, ANNOTATIONS_DIRECTORY_NAME),\n",
    "        videos_dir=os.path.join(DATASET_PATH, VIDEOS_FRAMES_DIRECTORY_NAME),\n",
    "        ids_file=os.path.join(DATASET_PATH, ANNOTATED_IDS_FILE_NAME),\n",
    "        segment_size=segment_size,\n",
    "        step=step,\n",
    "        video_processor=VideoFromVideoFramesDirectory,\n",
    "        annotations_processor=AnnotationsFromSegmentLevelCsvFileAnnotations,\n",
    "        annotations_processor_kwargs={\"fps\": 25, \"delimiter\": \",\"},\n",
    "        video_shape=(VideoShapeComponents.CHANNELS, VideoShapeComponents.TIME, VideoShapeComponents.HEIGHT, VideoShapeComponents.WIDTH),\n",
    "        frames_transform=extractor.transform_and_extract,\n",
    "        annotations_transform=__aggregate_labels,\n",
    "        verbose=False,\n",
    "        return_transform=returns_transform\n",
    "    )\n",
    "\n",
    "    print(f\"[extractor-{extractor.get_name()}]:\")\n",
    "\n",
    "    disk_cached_dataset = DiskCachedDataset.load_dataset_or_cache_it(\n",
    "        dataset=dataset, \n",
    "        base_path=os.path.join(DATASET_PATH, \"features\", extractor.get_name()),\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    datasets.append(disk_cached_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### **3- Data Filtering**\n",
    "\n",
    "Now we are going to filter the dataset(s) and get rid of the segments were:\n",
    "- The \"nothing\" class is present, meaning they are not annotated.\n",
    "- No person is present in more than half the segment.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_dataset_index = FEATURES_EXTRACTORS.index(next(filter(lambda x: x.get_name() == \"yolo\", FEATURES_EXTRACTORS)))\n",
    "\n",
    "yolo_dataset = datasets[yolo_dataset_index]\n",
    "\n",
    "# --- --- ---\n",
    "\n",
    "def extract_segments_without_classless_indices(yolo_dataset, classes: list[str]):\n",
    "    label_encoder = LabelEncoderFactory.get()\n",
    "    \n",
    "    return [i for i in range(len(yolo_dataset)) if yolo_dataset[i][1] not in label_encoder.transform(classes)]\n",
    "\n",
    "def extract_segments_with_persons_indices(yolo_dataset):\n",
    "    label_encoder = LabelEncoderFactory.get()\n",
    "    \n",
    "    # NOTE: we chose 4 as we take a sample of 32 frames, we then subsample 8 and extract the yolo features from them, thus 4 is the half of 8\n",
    "    return [i for i in range(len(dataset)) if torch.count_nonzero(torch.sum(yolo_dataset[i][0], dim=1)) >= 4]\n",
    "\n",
    "# NOTE: this is the indices to keep\n",
    "filtered_indices = list(set(extract_segments_without_classless_indices(yolo_dataset, [\"nothing\"]) + extract_segments_with_persons_indices(yolo_dataset)))\n",
    "\n",
    "# --- --- ---\n",
    "\n",
    "filtered_datasets = [torch.utils.data.Subset(dataset, filtered_indices) for dataset in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[len(datasets[0])]: 4098\n",
      "[len(filtered_datasets[0])]: 3995\n",
      "[difference]: 103\n",
      "--- --- ---\n",
      "[percentage]: 2.51%\n"
     ]
    }
   ],
   "source": [
    "print(f\"[len(datasets[0])]: {len(datasets[0])}\")\n",
    "print(f\"[len(filtered_datasets[0])]: {len(filtered_datasets[0])}\")\n",
    "print(f\"[difference]: {len(datasets[0]) - len(filtered_datasets[0])}\")\n",
    "print(f\"--- --- ---\")\n",
    "print(f\"[percentage]: {(len(datasets[0]) - len(filtered_datasets[0])) / len(datasets[0]) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullVideoFeaturesDataset():\n",
    "    def __init__(self, dataset: VideoDataset, transform=None, verbose=True):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.videos_segments_indices = self.__get_videos_segments_indices()\n",
    "    \n",
    "    def __get_videos_segments_indices(self):\n",
    "        if hasattr(self.dataset, '_cached_videos_segments_indices'):\n",
    "            return self.dataset._cached_videos_segments_indices\n",
    "\n",
    "        videos_segments_indices = {}\n",
    "        iterator = tqdm.tqdm(range(len(self.dataset))) if self.verbose else range(len(self.dataset))\n",
    "        \n",
    "        for sample_index in iterator:\n",
    "            _, _, video_id, segment_index = self.dataset[sample_index]\n",
    "            \n",
    "            if video_id not in videos_segments_indices:\n",
    "                videos_segments_indices[video_id] = [sample_index]\n",
    "            else:\n",
    "                videos_segments_indices[video_id].append(sample_index)\n",
    "        \n",
    "        for video_id in videos_segments_indices:\n",
    "            videos_segments_indices[video_id] = sorted(\n",
    "                videos_segments_indices[video_id], \n",
    "                key=lambda sample_index: self.dataset[sample_index][3]\n",
    "            )\n",
    "        \n",
    "        self.dataset._cached_videos_segments_indices = videos_segments_indices\n",
    "        return videos_segments_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.videos_segments_indices.keys())\n",
    "    \n",
    "    def __getitem__(self, video_index):\n",
    "        video_id = list(self.videos_segments_indices.keys())[video_index]\n",
    "        video_segments_indices = self.videos_segments_indices[video_id]\n",
    "\n",
    "        features = []\n",
    "        annotations = []\n",
    "\n",
    "        for sample_index in video_segments_indices:\n",
    "            frames, annotation, _, _ = self.dataset[sample_index]\n",
    "            features.append(frames)\n",
    "            annotations.append(annotation)\n",
    "            \n",
    "        if self.transform is not None:\n",
    "            return self.transform(features, annotations, video_id)\n",
    "        else:\n",
    "            return features, annotations, video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_features_dataset = DiskCachedDataset.load_dataset_or_cache_it(\n",
    "    dataset=dataset,\n",
    "    base_path=os.path.join(DATASET_PATH, \"augmented-features\", \"yolo-features\"),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# --- --- ---\n",
    "\n",
    "label_encoder = LabelEncoderFactory.get()\n",
    "\n",
    "yolo_dataset_index = FEATURES_EXTRACTORS.index(next(filter(lambda x: x[\"name\"] == \"yolo\", FEATURES_EXTRACTORS)))\n",
    "\n",
    "yolo_dataset = DiskCachedDataset(\n",
    "    base_path=os.path.join(DATASET_PATH, \"augmented-features\", \"yolo-features\"),\n",
    ")\n",
    "\n",
    "# --- --- ---\n",
    "\n",
    "def extract_segments_without_classless_indices(yolo_dataset, classes: list[str]):\n",
    "    label_encoder = LabelEncoderFactory.get()\n",
    "    \n",
    "    return [i for i in range(len(yolo_dataset)) if yolo_dataset[i][1] not in label_encoder.transform(classes)]\n",
    "\n",
    "def extract_segments_with_persons_indices(yolo_dataset):\n",
    "    label_encoder = LabelEncoderFactory.get()\n",
    "    \n",
    "    return [i for i in range(len(dataset)) if torch.count_nonzero(torch.sum(yolo_dataset[i][0], dim=1)) >= 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3995/3995 [00:00<00:00, 4990.08it/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_indices = list(set(extract_segments_without_classless_indices(yolo_dataset, [\"nothing\"]) + extract_segments_with_persons_indices(yolo_dataset)))\n",
    "\n",
    "videos = FullVideoFeaturesDataset(\n",
    "    dataset=torch.utils.data.Subset(disk_cached_dataset, filtered_indices),\n",
    "    transform=lambda features, annotations, video_id: (torch.stack(features), torch.tensor(np.array(annotations)[:, 0]), video_id),\n",
    "    verbose=True    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[len(videos)]: 22\n",
      "--- --- ---\n",
      "[video_id]: climb_9-climber_MasseQuentin-bloc_1-angle_face\n",
      "[features.shape]: torch.Size([173, 2048])\n",
      "[len(annotations)]: torch.Size([173])\n"
     ]
    }
   ],
   "source": [
    "print(f\"[len(videos)]: {len(videos)}\")\n",
    "\n",
    "print(f\"--- --- ---\")\n",
    "\n",
    "video_index = 0\n",
    "\n",
    "features, annotations, video_id = videos[video_index]\n",
    "\n",
    "print(f\"[video_id]: {video_id}\")\n",
    "print(f\"[features.shape]: {features.shape}\")\n",
    "print(f\"[len(annotations)]: {annotations.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloballyTemporalAwareModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size: The feature size for each time step.\n",
    "        hidden_size: The number of hidden units in the LSTM.\n",
    "        output_size: The number of classes.\n",
    "        num_layers: The number of LSTM layers. Default is 1.\n",
    "        dropout: The dropout probability. Default is 0.0.\n",
    "        \"\"\"\n",
    "        super(GloballyTemporalAwareModel, self).__init__()\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (final_hidden_states, final_cell_states) = self.lstm(x, None)\n",
    "        \n",
    "        output = self.fc(lstm_out)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[video.shape]: torch.Size([1, 180, 128])\n",
      "[model(video).shape]: torch.Size([1, 180, 5])\n"
     ]
    }
   ],
   "source": [
    "model = GloballyTemporalAwareModel(128, 100, 5)\n",
    "\n",
    "video = torch.zeros((1, 180, 128))\n",
    "\n",
    "print(f\"[video.shape]: {video.shape}\")\n",
    "\n",
    "output = model(video)\n",
    "\n",
    "print(f\"[model(video).shape]: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split sizes\n",
    "train_size = int(0.7 * len(videos))\n",
    "val_size = len(videos) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_videos, val_videos = torch.utils.data.random_split(videos, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_videos, batch_size=1, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_videos, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for features, annotations, _ in train_loader:\n",
    "        features, annotations = features.to(device), annotations.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(features)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output.view(-1, output.size(-1)), annotations.view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(output, -1)\n",
    "        correct_predictions += (predicted.view(-1) == annotations.view(-1)).sum().item()\n",
    "        total_predictions += annotations.numel()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, annotations, _ in val_loader:\n",
    "            features, annotations = features.to(device), annotations.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(features)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output.view(-1, output.size(-1)), annotations.view(-1))\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(output, -1)\n",
    "            correct_predictions += (predicted.view(-1) == annotations.view(-1)).sum().item()\n",
    "            total_predictions += annotations.numel()\n",
    "\n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "Train Loss: 1.4450, Train Accuracy: 32.46%\n",
      "Validation Loss: 1.4717, Validation Accuracy: 34.10%\n",
      "Epoch 2/32\n",
      "Train Loss: 1.4060, Train Accuracy: 34.87%\n",
      "Validation Loss: 1.4194, Validation Accuracy: 34.10%\n",
      "Epoch 3/32\n",
      "Train Loss: 1.3418, Train Accuracy: 51.56%\n",
      "Validation Loss: 1.3300, Validation Accuracy: 51.96%\n",
      "Epoch 4/32\n",
      "Train Loss: 1.1888, Train Accuracy: 57.46%\n",
      "Validation Loss: 1.2516, Validation Accuracy: 51.19%\n",
      "Epoch 5/32\n",
      "Train Loss: 1.1000, Train Accuracy: 60.13%\n",
      "Validation Loss: 1.1343, Validation Accuracy: 55.97%\n",
      "Epoch 6/32\n",
      "Train Loss: 1.0252, Train Accuracy: 62.43%\n",
      "Validation Loss: 1.2163, Validation Accuracy: 53.27%\n",
      "Epoch 7/32\n",
      "Train Loss: 1.0032, Train Accuracy: 62.31%\n",
      "Validation Loss: 1.0949, Validation Accuracy: 58.28%\n",
      "Epoch 8/32\n",
      "Train Loss: 0.9996, Train Accuracy: 61.50%\n",
      "Validation Loss: 1.0743, Validation Accuracy: 58.97%\n",
      "Epoch 9/32\n",
      "Train Loss: 0.9073, Train Accuracy: 65.10%\n",
      "Validation Loss: 1.0754, Validation Accuracy: 59.82%\n",
      "Epoch 10/32\n",
      "Train Loss: 0.8663, Train Accuracy: 65.65%\n",
      "Validation Loss: 1.0615, Validation Accuracy: 60.20%\n",
      "Epoch 11/32\n",
      "Train Loss: 0.8190, Train Accuracy: 68.58%\n",
      "Validation Loss: 1.0631, Validation Accuracy: 61.89%\n",
      "Epoch 12/32\n",
      "Train Loss: 0.7595, Train Accuracy: 73.59%\n",
      "Validation Loss: 1.0671, Validation Accuracy: 63.97%\n",
      "Epoch 13/32\n",
      "Train Loss: 0.7161, Train Accuracy: 75.67%\n",
      "Validation Loss: 1.0405, Validation Accuracy: 66.20%\n",
      "Epoch 14/32\n",
      "Train Loss: 0.7045, Train Accuracy: 75.85%\n",
      "Validation Loss: 1.2098, Validation Accuracy: 60.66%\n",
      "Epoch 15/32\n",
      "Train Loss: 0.7483, Train Accuracy: 73.15%\n",
      "Validation Loss: 1.0517, Validation Accuracy: 65.82%\n",
      "Epoch 16/32\n",
      "Train Loss: 0.7181, Train Accuracy: 74.48%\n",
      "Validation Loss: 1.0297, Validation Accuracy: 66.05%\n",
      "Epoch 17/32\n",
      "Train Loss: 0.6509, Train Accuracy: 78.23%\n",
      "Validation Loss: 1.0262, Validation Accuracy: 66.67%\n",
      "Epoch 18/32\n",
      "Train Loss: 0.6315, Train Accuracy: 78.00%\n",
      "Validation Loss: 1.0966, Validation Accuracy: 67.21%\n",
      "Epoch 19/32\n",
      "Train Loss: 0.6129, Train Accuracy: 78.78%\n",
      "Validation Loss: 1.0554, Validation Accuracy: 67.82%\n",
      "Epoch 20/32\n",
      "Train Loss: 0.6350, Train Accuracy: 78.38%\n",
      "Validation Loss: 1.2079, Validation Accuracy: 62.12%\n",
      "Epoch 21/32\n",
      "Train Loss: 0.7784, Train Accuracy: 73.22%\n",
      "Validation Loss: 1.0339, Validation Accuracy: 64.97%\n",
      "Epoch 22/32\n",
      "Train Loss: 0.6341, Train Accuracy: 78.00%\n",
      "Validation Loss: 1.0445, Validation Accuracy: 66.13%\n",
      "Epoch 23/32\n",
      "Train Loss: 0.6568, Train Accuracy: 76.85%\n",
      "Validation Loss: 1.1887, Validation Accuracy: 64.20%\n",
      "Epoch 24/32\n",
      "Train Loss: 0.8925, Train Accuracy: 68.69%\n",
      "Validation Loss: 1.0766, Validation Accuracy: 63.74%\n",
      "Epoch 25/32\n",
      "Train Loss: 0.6776, Train Accuracy: 76.15%\n",
      "Validation Loss: 1.0277, Validation Accuracy: 68.44%\n",
      "Epoch 26/32\n",
      "Train Loss: 0.6234, Train Accuracy: 79.12%\n",
      "Validation Loss: 1.1409, Validation Accuracy: 66.59%\n",
      "Epoch 27/32\n",
      "Train Loss: 0.6514, Train Accuracy: 78.19%\n",
      "Validation Loss: 1.1013, Validation Accuracy: 67.13%\n",
      "Epoch 28/32\n",
      "Train Loss: 0.5916, Train Accuracy: 79.49%\n",
      "Validation Loss: 1.1296, Validation Accuracy: 68.05%\n",
      "Epoch 29/32\n",
      "Train Loss: 0.5428, Train Accuracy: 81.34%\n",
      "Validation Loss: 1.0907, Validation Accuracy: 68.90%\n",
      "Epoch 30/32\n",
      "Train Loss: 0.5091, Train Accuracy: 82.53%\n",
      "Validation Loss: 1.1583, Validation Accuracy: 68.28%\n",
      "Epoch 31/32\n",
      "Train Loss: 0.5033, Train Accuracy: 82.42%\n",
      "Validation Loss: 1.1437, Validation Accuracy: 68.67%\n",
      "Epoch 32/32\n",
      "Train Loss: 0.5015, Train Accuracy: 82.20%\n",
      "Validation Loss: 1.1183, Validation Accuracy: 68.75%\n",
      "\n",
      "Best Validation Accuracy: 68.90%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 2048\n",
    "hidden_size = 128\n",
    "output_size = 5\n",
    "learning_rate = 0.001\n",
    "num_epochs = 32\n",
    "num_layers = 4\n",
    "dropout = 0.0\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GloballyTemporalAwareModel(input_size, hidden_size, output_size, num_layers, dropout).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training and validation loop with best model tracking\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Train the model\n",
    "    train_loss, train_acc = train_model(model, train_loader, optimizer, criterion, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "    # Validate the model\n",
    "    val_loss, val_acc = validate_model(model, val_loader, criterion, device)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    # Track the best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict()\n",
    "\n",
    "print(f\"\\nBest Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "# Optionally save the best model\n",
    "torch.save(best_model_state, \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
