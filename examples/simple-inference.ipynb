{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = \"/Users/nadir/Documents/research-project-example-videos/climb_1-climber_MoubeAdrian-bloc_1-angle_face.mp4\"\n",
    "SEGMENT_SIZE = 32\n",
    "NUMBER_OF_CLASSES = 5\n",
    "\n",
    "VIDEO_SEGMENT_MLP_MODEL_WEIGHTS_PATH = \"models-weights/mlp.x3d-xs.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from bouldering_video_segmentation.models import VideoSegmentMlp\n",
    "from bouldering_video_segmentation.inference import segment_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/nadir/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "[processing-video-segments]::   1%|‚ñè         | 3/221 [00:02<02:51,  1.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m segments \u001b[38;5;241m=\u001b[39m \u001b[43msegment_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVIDEO_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/bouldering_video_segmentation/inference.py:28\u001b[0m, in \u001b[0;36msegment_video\u001b[0;34m(video_path, feature_extractor, classifier, verbose)\u001b[0m\n\u001b[1;32m     25\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(iterable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(video), SEGMENT_SIZE), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[processing-video-segments]:\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m verbose):\n\u001b[0;32m---> 28\u001b[0m     segment \u001b[38;5;241m=\u001b[39m \u001b[43mvideo\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mSEGMENT_SIZE\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# NOTE: required to be transposed to (Channel, Time, Height, Width)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     segment \u001b[38;5;241m=\u001b[39m segment\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/code/.venv/lib/python3.12/site-packages/video_dataset/video.py:137\u001b[0m, in \u001b[0;36mVideoFromVideoFile.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m    136\u001b[0m     start, stop, step \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mindices(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m())\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex must be an integer or slice\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/code/.venv/lib/python3.12/site-packages/video_dataset/video.py:166\u001b[0m, in \u001b[0;36mVideoFromVideoFile.__get_frames\u001b[0;34m(self, start, stop, step)\u001b[0m\n\u001b[1;32m    162\u001b[0m video\u001b[38;5;241m.\u001b[39mset(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_POS_FRAMES, start)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start, stop, step):\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# NOTE: return in the shape (height, width, channels)\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mvideo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier = VideoSegmentMlp(\n",
    "    num_classes=NUMBER_OF_CLASSES,\n",
    "    segment_size=SEGMENT_SIZE\n",
    ")\n",
    "classifier.load_state_dict(\n",
    "    torch.load(VIDEO_SEGMENT_MLP_MODEL_WEIGHTS_PATH)\n",
    ")\n",
    "classifier.eval()\n",
    "\n",
    "segments = segment_video(\n",
    "    video_path=VIDEO_PATH,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
